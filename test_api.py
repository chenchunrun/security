#!/usr/bin/env python3
# Copyright 2026 CCR <chenchunrun@gmail.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""æµ‹è¯•LLM APIè¿æ¥"""
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from langchain_openai import ChatOpenAI
from src.utils.config import config

# Load environment variables
load_dotenv()

print("=" * 80)
print("ğŸ” LLM API Connection Test")
print("=" * 80)
print()

# æ˜¾ç¤ºå½“å‰é…ç½®
print("ğŸ“‹ Current Configuration:")
print(f"   Model:        {config.llm_model}")
print(f"   Temperature:  {config.llm_temperature}")
print(f"   API Key:      {config.llm_api_key[:10]}...{config.llm_api_key[-4:] if config.llm_api_key else 'NOT SET'}")
print(f"   Base URL:     {config.llm_base_url or 'OpenAI Default'}")
print()

# Check if API key is set
if not config.llm_api_key:
    print("âŒ LLM_API_KEY not set!")
    print()
    print("Please configure your API key in .env file:")
    print()
    print("For Qwen (é€šä¹‰åƒé—®):")
    print("  LLM_API_KEY=sk-your-qwen-key")
    print("  LLM_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1")
    print()
    print("For OpenAI:")
    print("  LLM_API_KEY=sk-your-openai-key")
    print("  LLM_BASE_URL=")
    sys.exit(1)

# Test connection
print("ğŸ”Œ Testing API connection...")
print()

try:
    # Initialize LLM
    llm = ChatOpenAI(
        model=config.llm_model,
        temperature=config.llm_temperature,
        api_key=config.llm_api_key,
        base_url=config.llm_base_url,
        timeout=config.get("agents.timeout", 300)
    )

    # Send test message
    print("ğŸ“¤ Sending test message...")
    test_message = "ä½ å¥½ï¼è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"

    response = llm.invoke(test_message)

    print()
    print("âœ… API Connection Successful!")
    print()
    print(f"ğŸ“¥ Response:")
    print(f"   {response.content}")
    print()
    print("=" * 80)
    print("âœ… Your API is configured correctly!")
    print("=" * 80)
    print()
    print("Next steps:")
    print("  1. Run: python3 test_system.py  # Test system components")
    print("  2. Run: python3 main.py --sample # Run full system")
    print()

except Exception as e:
    print()
    print("âŒ API Connection Failed!")
    print()
    print(f"Error: {str(e)}")
    print()
    print("Troubleshooting:")
    print("  1. Check your API key in .env file")
    print("  2. Verify base URL is correct")
    print("  3. Check your network connection")
    print("  4. Ensure you have sufficient API quota")
    print()
    print("For detailed help, see: LLM_API_CONFIG.md")
    sys.exit(1)
